{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "01c5e254",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Core libraries for JSON/SQL/CSV analysis\n",
        "import json\n",
        "import sys\n",
        "import pandas as pd\n",
        "import sqlite3\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "from pathlib import Path\n",
        "\n",
        "# Add project root to path (for running from notebooks/ or project root)\n",
        "_root = Path.cwd().parent if (Path.cwd().parent / \"data\" / \"raw\").exists() else Path.cwd()\n",
        "if str(_root) not in sys.path:\n",
        "    sys.path.insert(0, str(_root))\n",
        "from config import DATA_RAW, DATA_PROCESSED, DB_PATH"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "530f8ae0",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Found 12 ndjson files\n",
            "- AllergyIntolerance.000.ndjson\n",
            "- Condition.000.ndjson\n",
            "- Device.000.ndjson\n",
            "- DiagnosticReport.000.ndjson\n",
            "- DocumentReference.000.ndjson\n",
            "- Encounter.000.ndjson\n",
            "- Immunization.000.ndjson\n",
            "- MedicationRequest.000.ndjson\n",
            "- Observation.000.ndjson\n",
            "- Observation.001.ndjson\n",
            "- Patient.000.ndjson\n",
            "- Procedure.000.ndjson\n",
            "\n",
            "AllergyIntolerance.000.ndjson rows: 75 cols: 12\n",
            "\n",
            "Condition.000.ndjson rows: 4559 cols: 12\n",
            "\n",
            "Device.000.ndjson rows: 208 cols: 13\n",
            "\n",
            "DiagnosticReport.000.ndjson rows: 15747 cols: 13\n",
            "\n",
            "DocumentReference.000.ndjson rows: 9069 cols: 13\n",
            "\n",
            "Encounter.000.ndjson rows: 9069 cols: 14\n",
            "\n",
            "Immunization.000.ndjson rows: 1818 cols: 10\n",
            "\n",
            "MedicationRequest.000.ndjson rows: 10367 cols: 14\n",
            "\n",
            "Observation.000.ndjson rows: 52820 cols: 14\n",
            "\n",
            "Observation.001.ndjson rows: 12727 cols: 14\n",
            "\n",
            "Patient.000.ndjson rows: 120 cols: 16\n",
            "\n",
            "Procedure.000.ndjson rows: 14085 cols: 11\n"
          ]
        }
      ],
      "source": [
        "# Quick inventory of NDJSON files and basic shape\n",
        "ndjson_files = sorted(DATA_RAW.glob(\"*.ndjson\"))\n",
        "\n",
        "print(f\"Found {len(ndjson_files)} ndjson files\")\n",
        "for path in ndjson_files:\n",
        "    print(\"-\", path.name)\n",
        "\n",
        "# Load each ndjson file into a dataframe\n",
        "dfs = {}\n",
        "for path in ndjson_files:\n",
        "    df = pd.read_json(path, lines=True)\n",
        "    dfs[path.name] = df\n",
        "\n",
        "# Print head of each dataframe\n",
        "for name, df in dfs.items():\n",
        "    print(f\"\\n{name} rows: {len(df)} cols: {len(df.columns)}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "id": "5a18b2d4",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Loaded 75 rows into 'allergyintolerance'\n",
            "Loaded 4559 rows into 'condition'\n",
            "Loaded 208 rows into 'device'\n",
            "Loaded 15747 rows into 'diagnosticreport'\n",
            "Loaded 9069 rows into 'documentreference'\n",
            "Loaded 9069 rows into 'encounter'\n",
            "Loaded 1818 rows into 'immunization'\n",
            "Loaded 10367 rows into 'medicationrequest'\n",
            "Loaded 52820 rows into 'observation'\n",
            "Loaded 12727 rows into 'observation_1'\n",
            "Loaded 120 rows into 'patient'\n",
            "Loaded 14085 rows into 'procedure'\n",
            "Created 'observation_union' with 237 columns\n"
          ]
        }
      ],
      "source": [
        "# Flatten NDJSON into SQLite tables for SQL queries\n",
        "ndjson_files = sorted(DATA_RAW.glob(\"*.ndjson\"))\n",
        "\n",
        "# Flatten nested dict/list structures\n",
        "def _flatten_value(value, prefix, out):\n",
        "    if isinstance(value, dict):\n",
        "        for k, v in value.items():\n",
        "            _flatten_value(v, f\"{prefix}.{k}\" if prefix else k, out)\n",
        "    elif isinstance(value, list):\n",
        "        for i, v in enumerate(value):\n",
        "            _flatten_value(v, f\"{prefix}[{i}]\", out)\n",
        "        if len(value) == 0:\n",
        "            out[prefix] = None\n",
        "    else:\n",
        "        out[prefix] = value\n",
        "\n",
        "def flatten_record(record):\n",
        "    out = {}\n",
        "    _flatten_value(record, \"\", out)\n",
        "    return out\n",
        "\n",
        "# Load NDJSON files into SQLite\n",
        "DATA_PROCESSED.mkdir(parents=True, exist_ok=True)\n",
        "conn = sqlite3.connect(DB_PATH)\n",
        "name_counts = {}\n",
        "\n",
        "for path in ndjson_files:\n",
        "    with path.open(\"r\", encoding=\"utf-8\") as f:\n",
        "        records = [flatten_record(json.loads(line)) for line in f if line.strip()]\n",
        "\n",
        "    df = pd.DataFrame(records)\n",
        "\n",
        "    base_name = path.name.split(\".\", 1)[0].lower() ## normalize file names\n",
        "    count = name_counts.get(base_name, 0)\n",
        "    table_name = f\"{base_name}_{count}\" if count > 0 else base_name ## create unique table names for duplicate file names\n",
        "    name_counts[base_name] = count + 1 ## increment count for next table\n",
        "\n",
        "    df.to_sql(table_name, conn, if_exists=\"replace\", index=False)\n",
        "    print(f\"Loaded {len(df)} rows into '{table_name}'\")\n",
        "\n",
        "# Combine observation files into one union table\n",
        "obs_cols_query = \"PRAGMA table_info(observation);\"\n",
        "obs_cols = [row[1] for row in conn.execute(obs_cols_query).fetchall()]\n",
        "\n",
        "obs1_cols_query = \"PRAGMA table_info(observation_1);\"\n",
        "obs1_cols = [row[1] for row in conn.execute(obs1_cols_query).fetchall()]\n",
        "\n",
        "common_cols = [col for col in obs_cols if col in obs1_cols]\n",
        "\n",
        "union_cols_str = \", \".join([f'\"{col}\"' for col in common_cols])\n",
        "\n",
        "union_table_sql = f\"\"\"\n",
        "CREATE TABLE IF NOT EXISTS observation_union AS\n",
        "SELECT * FROM observation\n",
        "UNION ALL\n",
        "SELECT * FROM observation_1;\n",
        "\"\"\"\n",
        "conn.execute(union_table_sql)\n",
        "conn.commit()\n",
        "print(f\"Created 'observation_union' with {len(common_cols)} columns\")\n",
        "\n",
        "conn.close()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "id": "499435e3",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "allergyintolerance.csv columns (11):\n",
            "['patient_id', 'allergy_type', 'allergy_category', 'allergy_date', 'allergy_name', 'allergy_reaction_1', 'allergy_reaction_2', 'allergy_reaction_3', 'allergy_reaction_4', 'allergy_reaction_5', 'allergy_reaction_6']\n",
            "\n",
            "condition.csv columns (7):\n",
            "['patient_id', 'encounter_id', 'condition_id', 'condition', 'condition_resolved', 'condition_onset_date', 'condition_abatement_date']\n",
            "\n",
            "device.csv columns (4):\n",
            "['patient_id', 'device_expiration_date', 'device_name', 'device_expired']\n",
            "\n",
            "encounter.csv columns (7):\n",
            "['patient_id', 'encounter_id', 'encounter_type', 'encounter_start_date', 'encounter_end_date', 'encounter_length_hours', 'encounter_reason']\n",
            "\n",
            "immunization.csv columns (4):\n",
            "['patient_id', 'encounter_id', 'vaccine_name', 'vaccine_date']\n",
            "\n",
            "medicationrequest.csv columns (4):\n",
            "['patient_id', 'encounter_id', 'status', 'medication_name']\n",
            "\n",
            "observation.csv columns (48):\n",
            "['patient_id', 'encounter_id', 'observation_date', 'observation_category', 'observation_name', 'value', 'component_0_name', 'component_0_value', 'component_1_name', 'component_1_value', 'component_2_name', 'component_2_value', 'component_3_name', 'component_3_value', 'component_4_name', 'component_4_value', 'component_5_name', 'component_5_value', 'component_6_name', 'component_6_value', 'component_7_name', 'component_7_value', 'component_8_name', 'component_8_value', 'component_9_name', 'component_9_value', 'component_10_name', 'component_10_value', 'component_11_name', 'component_11_value', 'component_12_name', 'component_12_value', 'component_13_name', 'component_13_value', 'component_14_name', 'component_14_value', 'component_15_name', 'component_15_value', 'component_16_name', 'component_16_value', 'component_17_name', 'component_17_value', 'component_18_name', 'component_18_value', 'component_19_name', 'component_19_value', 'component_20_name', 'component_20_value']\n",
            "\n",
            "patients.csv columns (20):\n",
            "['patient_id', 'state', 'country', 'birth_date', 'deceased_date', 'age', 'race', 'hispanic', 'gender', 'disability_adjust_life_years', 'quality_adjusted_life_years', 'marital_status', 'deceased', 'age_at_death', 'us_resident', 'multiple_birth', 'DALY_norm', 'QALY_norm', 'DALY_inv', 'HealthIndex']\n",
            "\n",
            "procedure.csv columns (9):\n",
            "['patient_id', 'encounter_id', 'condition_id', 'status', 'procedure_name', 'procedure_start_date', 'procedure_end_date', 'procedure_length_hours', 'reason_for_procedure']\n"
          ]
        }
      ],
      "source": [
        "# Inspect exported CSV schemas for Tableau\n",
        "csv_files = sorted(DATA_PROCESSED.glob(\"*.csv\"))\n",
        "\n",
        "columns_map = {}\n",
        "for path in csv_files:\n",
        "    df = pd.read_csv(path, nrows=1)\n",
        "    columns_map[path.name] = list(df.columns)\n",
        "\n",
        "for name, cols in columns_map.items():\n",
        "    print(f\"\\n{name} columns ({len(cols)}):\")\n",
        "    print(cols)"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "base",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.7"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
